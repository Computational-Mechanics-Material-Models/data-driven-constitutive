{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c9e3d6-c8b5-4341-b7a1-295c2c7f243c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a7830c-5347-4ba1-8138-c69948cfb73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645ee226-b7c1-4201-9b3a-bd718fe153da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Check available GPUs\n",
    "physical_devices = torch.cuda.device_count()\n",
    "print(f\"Available GPUs: {physical_devices}\")\n",
    "\n",
    "# Set the GPU to use\n",
    "gpu_id = 4  # Choose the desired GPU index\n",
    "\n",
    "# Ensure the selected GPU exists\n",
    "if gpu_id < physical_devices:\n",
    "    device = torch.device(f\"cuda:{gpu_id}\")\n",
    "    torch.cuda.set_device(device)  # Set the current device\n",
    "    print(f\"Binding to GPU {gpu_id}: {torch.cuda.get_device_name(gpu_id)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"GPU {gpu_id} not available, using CPU instead.\")\n",
    "\n",
    "# Example: Move a tensor to the selected GPU\n",
    "tensor_example = torch.tensor([1.0, 2.0, 3.0]).to(device)\n",
    "print(tensor_example.device)  # Should print \"cuda:4\" if GPU 4 is available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac6f311-0770-4745-bd4f-0563c737791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 文件路径\n",
    "file_paths = [\n",
    "\n",
    "    \"averaged_size_30_strain22.csv\",\n",
    "\n",
    "]\n",
    "\n",
    "# 读取数据\n",
    "df_list = [pd.read_csv(file) for file in file_paths]\n",
    "\n",
    "# 逐个增加索引\n",
    "index_offset = 0\n",
    "for df in df_list:\n",
    "    df['index'] = df['index'] + index_offset\n",
    "    index_offset += len(df['index'].unique())\n",
    "\n",
    "# 合并数据\n",
    "df_combined = pd.concat(df_list, ignore_index=False)\n",
    "\n",
    "# 删除缺失值\n",
    "df_combined.dropna(inplace=True)\n",
    "\n",
    "# 转换数据类型为 float64\n",
    "df_combined = df_combined.astype(np.float64)\n",
    "\n",
    "# 显示前 1001 行\n",
    "print(df_combined.head(1001))\n",
    "df_combined.to_csv(\"combined_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e42241-46f5-4b8b-8438-b854d62f0e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "angle1 = np.genfromtxt(\"angle1.txt\", delimiter=',')\n",
    "angle2 = np.genfromtxt(\"angle2.txt\", delimiter=',')\n",
    "angle3 = np.genfromtxt(\"angle3.txt\", delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bc42f8-811d-4c98-aea2-d20af1620435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateRmatrix(angle1, angle2, angle3):\n",
    "    R1 = np.array([[np.cos(angle1), -np.sin(angle1), 0],[np.sin(angle1), np.cos(angle1), 0],[0, 0, 1]])\n",
    "    print(R1.shape)\n",
    "    R2 = np.array([[np.cos(angle2), 0, np.sin(angle2)], [0,1,0], [-np.sin(angle2), 0, np.cos(angle2)]])\n",
    "    R3 = np.array([[1, 0, 0], [0, np.cos(angle3), -np.sin(angle3)], [0, np.sin(angle3), np.cos(angle3)]])\n",
    "    R = np.matmul(np.matmul(R1, R2), R3)\n",
    "    return R\n",
    "\n",
    "R=generateRmatrix(angle1[0], angle2[0], angle3[0])\n",
    "print(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d0c199-656f-400d-9e01-f7bfbef9aefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "sequence_length = 1000  # 每个序列的时间步\n",
    "input_n_features = 6\n",
    "output_n_features = 6\n",
    "\n",
    "input_columns = [\"strain11\", \"strain22\", \"strain33\", \"strain12\", \"strain13\", \"strain23\"]\n",
    "output_columns = [\"stress11\", \"stress22\", \"stress33\", \"stress12\", \"stress13\", \"stress23\"]\n",
    "\n",
    "# 重新计算符合条件的样本数\n",
    "valid_indices = df_combined['index'].unique()\n",
    "count = len(valid_indices)   # 每个 index 有 3 组数据\n",
    "\n",
    "# 初始化 X 和 y 数组\n",
    "X = np.zeros((count, sequence_length, input_n_features))\n",
    "y = np.zeros((count, sequence_length, output_n_features))\n",
    "\n",
    "# 填充 X 和 y\n",
    "count = 0\n",
    "for i in valid_indices:\n",
    "    df1 = df_combined[df_combined['index'] == i]\n",
    "    df1 = df1.sort_values(by=\"step\")  # 确保 step 顺序正确\n",
    "    \n",
    "    # 按 step 递增分成三组，每组取 1000 行\n",
    "    for j in range(3):\n",
    "        subset = df1.iloc[j * sequence_length : (j + 1) * sequence_length]\n",
    "        if len(subset) == sequence_length:\n",
    "            X[count] = subset[input_columns].to_numpy()\n",
    "            y[count] = subset[output_columns].to_numpy()\n",
    "            count += 1\n",
    "\n",
    "# 输出最终 X, y 的形状\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e895ff0-0e4a-489b-90d9-3997b08dd262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新计算符合条件的样本数\n",
    "count = len(valid_indices)   # 每个 index 有 3 组数据\n",
    "\n",
    "# 初始化 X 和 y 数组\n",
    "X = np.zeros((count, sequence_length, input_n_features))\n",
    "y = np.zeros((count, sequence_length, output_n_features))\n",
    "\n",
    "# 填充 X 和 y\n",
    "count = 0\n",
    "for i in valid_indices:\n",
    "    df1 = df_combined[df_combined['index'] == i]\n",
    "    df1 = df1.sort_values(by=\"step\")  # 确保 step 顺序正确\n",
    "\n",
    "    # 按 step 递增分成三组，每组取 1000 行\n",
    "    for j in range(3):\n",
    "        subset = df1.iloc[j * sequence_length : (j + 1) * sequence_length]\n",
    "        if len(subset) == sequence_length:\n",
    "            # 计算标准差\n",
    "            std_devs_in = subset[input_columns].std().to_numpy()\n",
    "            std_devs_out = subset[output_columns].std().to_numpy()\n",
    "\n",
    "            # 避免除零错误\n",
    "            std_devs_in[std_devs_in == 0] = 1e-6\n",
    "            std_devs_out[std_devs_out == 0] = 1e-6\n",
    "\n",
    "            #归一化数据\n",
    "            X[count] = subset[input_columns].to_numpy() / std_devs_in\n",
    "            y[count] = subset[output_columns].to_numpy() / std_devs_out\n",
    "           #  # Compute normalization parameters\n",
    "           #  X_min = subset[input_columns].min().to_numpy()  # Minimum values for each feature\n",
    "           #  X_max = subset[input_columns].max().to_numpy()  # Maximum values for each feature\n",
    "\n",
    "           #  X_m = (X_min + X_max) / 2  # Mean of min and max\n",
    "           #  X_s = (X_max - X_min) / 2  # Scaling factor\n",
    "\n",
    "           # # Normalize X using the given formula\n",
    "           #  X[count] = (subset[input_columns].to_numpy() - X_m) / X_s\n",
    "\n",
    "           # # Compute normalization parameters for y\n",
    "           #  y_min = subset[output_columns].min().to_numpy()\n",
    "           #  y_max = subset[output_columns].max().to_numpy()\n",
    "\n",
    "           #  y_m = (y_min + y_max) / 2\n",
    "           #  y_s = (y_max - y_min) / 2\n",
    "\n",
    "           #  # Normalize y using the given formula\n",
    "           #  y[count] = (subset[output_columns].to_numpy() - y_m) / y_s\n",
    "           #  # X[count] = subset[input_columns].to_numpy() \n",
    "           #  # y[count] = subset[output_columns].to_numpy() \n",
    "            # Compute normalization parameters\n",
    "            X_min = subset[input_columns].min().to_numpy()  # Minimum values for each feature\n",
    "            X_max = subset[input_columns].max().to_numpy()  # Maximum values for each feature\n",
    "\n",
    "          # Normalize X using min-max normalization\n",
    "            X[count] = (subset[input_columns].to_numpy() - X_min) / (X_max - X_min)\n",
    "\n",
    "         # Compute normalization parameters for y\n",
    "            y_min = subset[output_columns].min().to_numpy()\n",
    "            y_max = subset[output_columns].max().to_numpy()\n",
    "\n",
    "          # Normalize y using min-max normalization\n",
    "            y[count] = (subset[output_columns].to_numpy() - y_min) / (y_max - y_min)\n",
    "\n",
    "            count += 1\n",
    "\n",
    "# 输出最终 X, y 的形状\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6168259f-29dd-4b8e-bed5-535c7774f28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define custom loss function in PyTorch\n",
    "def make_custom_loss_batch(model, X_batch):\n",
    "    def custom_loss(y_pred, y_true):\n",
    "        # Convert R to a PyTorch tensor\n",
    "        R_tensor = torch.tensor(R, dtype=torch.float32, device=y_pred.device)\n",
    "        strain = X_batch.to(dtype=torch.float32)\n",
    "\n",
    "        # Extract the diagonal strain components (first three features)\n",
    "        diagonal_strain = strain[:, :, :3]  # Shape: (batch_size, 1000, 3)\n",
    "\n",
    "        # Compute Term 1: MSE loss between predictions and ground truth\n",
    "        term1 = torch.mean(torch.sum((y_pred - y_true) ** 2, dim=[1, 2]))\n",
    "\n",
    "        # Compute Term 2: Rotation-based transformation\n",
    "        rotated_strain = torch.matmul(diagonal_strain.view(-1, 3), R_tensor)\n",
    "        rotated_strain = rotated_strain.view(diagonal_strain.shape)  # Reshape to original shape\n",
    "\n",
    "        # Compute R^{-1} (inverse of R)\n",
    "        R_tensor_inv = torch.linalg.inv(R_tensor)  \n",
    "        transformed_strain = torch.matmul(rotated_strain.view(-1, 3), R_tensor_inv)\n",
    "        transformed_strain = transformed_strain.view(rotated_strain.shape)\n",
    "\n",
    "        # Pad transformed strain to match the input shape\n",
    "        transformed_strain_padded = torch.cat(\n",
    "            [transformed_strain, torch.zeros_like(strain[:, :, 3:])], dim=-1\n",
    "        )\n",
    "\n",
    "        # Predict stress using the model\n",
    "        predicted_transformed_stress = model(transformed_strain_padded)  \n",
    "\n",
    "        # Apply rotation matrix to stress\n",
    "        rotated_stress = torch.matmul(y_pred[:, :, :3].reshape(-1, 3), R_tensor)\n",
    "        rotated_stress = rotated_stress.view(y_pred[:, :, :3].shape)\n",
    "\n",
    "        # Compute Term 2 difference\n",
    "        difference = predicted_transformed_stress[:, :, :3] - rotated_stress\n",
    "        term2 = torch.mean(torch.sum(difference ** 2, dim=[1, 2]))\n",
    "\n",
    "        # Compute Term 3: Delta stress change\n",
    "        strain_current = strain[:, 1:, :6]  \n",
    "        strain_previous = strain[:, :-1, :6]  \n",
    "        delta_sigma = torch.cat(\n",
    "            [strain_previous[:, :1, :], strain_current - strain_previous], dim=1\n",
    "        )\n",
    "\n",
    "        # Compute stress dot product change\n",
    "        stress_dot_change = torch.sum(y_pred * delta_sigma, dim=[1, 2])\n",
    "        t = 1.0\n",
    "        relu_term = F.relu(-t * stress_dot_change)\n",
    "        term3 = torch.mean(relu_term)\n",
    "\n",
    "        # Print debug information\n",
    "        print(\"term1:\", term1.item(), \"term2:\", term2.item(), \"term3:\", term3.item(), \"sum:\", (term1 + term2 + term3).item())\n",
    "\n",
    "        return term1 + term3  # Return the final loss (excluding term2)\n",
    "\n",
    "    return custom_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff84077-d2e2-4930-b9ed-5e86f6c4f5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# # Simulate dataset (Replace with real data)\n",
    "num_samples, sequence_length, num_features = 100, 1000, 6\n",
    "# X = np.random.rand(num_samples, sequence_length, num_features).astype(np.float32)\n",
    "# y = np.random.rand(num_samples, 6).astype(np.float32)  # Output shape (batch_size, 6)\n",
    "\n",
    "# Convert dataset to PyTorch tensors\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "# Define LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, dropout1, dropout2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_dim, hidden_dim1, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(dropout1)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim1, hidden_dim2, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(dropout2)\n",
    "        self.fc = nn.Linear(hidden_dim2, 6)  # Output layer for regression\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"Input shape to LSTM: {x.shape}\")  # Debugging\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc(x) \n",
    "        print(f\"Output shape from LSTM: {x.shape}\")  # Debugging\n",
    "        return x\n",
    "\n",
    "# Define Optuna Objective Function\n",
    "def objective(trial):\n",
    "    # Sample hyperparameters\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 16,64, step=8)\n",
    "    lstm_units_1 = trial.suggest_int(\"lstm_units_1\", 32, 128, step=16)\n",
    "    lstm_units_2 = trial.suggest_int(\"lstm_units_2\", 16, 64, step=16)\n",
    "    dropout_1 = trial.suggest_float(\"dropout_1\", 0.1, 0.5, step=0.1)\n",
    "    dropout_2 = trial.suggest_float(\"dropout_2\", 0.1, 0.5, step=0.1)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n",
    "\n",
    "    # Create DataLoader (Fix 1: drop_last=True to ensure equal batch sizes)\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    # Initialize model\n",
    "    model = LSTMModel(input_dim=6, hidden_dim1=lstm_units_1, hidden_dim2=lstm_units_2,\n",
    "                      dropout1=dropout_1, dropout2=dropout_2).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    \n",
    "    # criterion = nn.MSELoss()  # Mean Squared Error for regression task\n",
    "\n",
    "    # # Debugging: Check first batch shapes\n",
    "    # for X_batch, y_batch in train_loader:\n",
    "    #     print(f\"Batch X shape: {X_batch.shape}\")  # Expected: (batch_size, sequence_length, num_features)\n",
    "    #     print(f\"Batch y shape: {y_batch.shape}\")  # Expected: (batch_size, 6)\n",
    "    #     break\n",
    "\n",
    "    # Training loop\n",
    "    epochs = 300\n",
    "    for epoch in range(epochs):\n",
    "        total_loss_epoch = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            # loss = criterion(y_pred, y_batch)\n",
    "            # Get the loss function dynamically for this batch\n",
    "            loss_fn = make_custom_loss_batch(model, X_batch)  \n",
    "            loss = loss_fn(y_pred, y_batch)  \n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss_epoch += loss.item()\n",
    "        print(f'Epoch {epoch + 1}, Total Loss: {total_loss_epoch}')\n",
    "    \n",
    "    return total_loss_epoch  # Return final loss for Optuna to minimize\n",
    "\n",
    "# Run Optuna Optimization\n",
    "study = optuna.create_study(direction=\"minimize\")  # Minimize the loss\n",
    "study.optimize(objective, n_trials=10)  # Reduce trials for debugging\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d21d9d8-08a3-4522-b2b6-3e94bea4ea1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Simulate dataset (Replace with real data)\n",
    "num_samples, sequence_length, num_features = 100, 1000, 6\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=40)\n",
    "\n",
    "\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "# Define LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, dropout1, dropout2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_dim, hidden_dim1, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(dropout1)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim1, hidden_dim2, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(dropout2)\n",
    "        self.fc = nn.Linear(hidden_dim2, 6)  # Output layer for regression\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc(x)  # Select last timestep for prediction\n",
    "        return x\n",
    "\n",
    "\n",
    "# def train_model(model, train_loader, optimizer, epochs):\n",
    "#     model.train()\n",
    "#     for epoch in range(epochs):\n",
    "#         total_loss_epoch = 0.0\n",
    "#         for X_batch, y_batch in train_loader:\n",
    "#             optimizer.zero_grad()\n",
    "#             y_pred = model(X_batch)\n",
    "#             loss = make_custom_loss_batch(y_pred, y_batch)  # Use custom loss\n",
    "#             # loss.backward()\n",
    "#             optimizer.step()\n",
    "#             total_loss_epoch += loss.item()\n",
    "#         print(f'Epoch {epoch + 1}/{epochs} - Total Loss: {total_loss_epoch}')\n",
    "        \n",
    "def train_model(model, train_loader, optimizer, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss_epoch = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "\n",
    "            # Get the loss function dynamically for this batch\n",
    "            loss_fn = make_custom_loss_batch(model, X_batch)  \n",
    "            loss = loss_fn(y_pred, y_batch)  \n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss_epoch += loss.item()\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}/{epochs} - Total Loss: {total_loss_epoch}')\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 56\n",
    "lstm_units_1 = 80\n",
    "lstm_units_2 = 64\n",
    "dropout_1 = 0.1\n",
    "dropout_2 = 0.3\n",
    "learning_rate = 0.005\n",
    "epochs = 1000  # Adjust for testing\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "# Initialize Model, Optimizer, Loss Function\n",
    "model = LSTMModel(input_dim=6, hidden_dim1=lstm_units_1, hidden_dim2=lstm_units_2,\n",
    "                  dropout1=dropout_1, dropout2=dropout_2).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "# Train Model\n",
    "train_model(model, train_loader, optimizer, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe5aad3-d872-4454-9783-e89e88575499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import os\n",
    "# from sklearn.metrics import r2_score\n",
    "\n",
    "# # Ensure model is in evaluation mode\n",
    "# model.eval()\n",
    "\n",
    "# # Create a directory to save plots\n",
    "# save_dir = 'test_set_plots_3'\n",
    "# os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# # print(X.shape, y.shape, X_test.shape, y_test.shape, X_train.shape, y_train.shape)\n",
    "\n",
    "\n",
    "# # Ensure X_test is a PyTorch tensor and move to the correct device\n",
    "# X_test = X_test.to(next(model.parameters()).device)\n",
    "\n",
    "# # print(X.shape, y.shape, X_test.shape, y_test.shape, X_train.shape, y_train.shape)\n",
    "\n",
    "\n",
    "# # Make predictions for the test set (disable gradients)\n",
    "# with torch.no_grad():\n",
    "#     predictions = model(X_test)  # Forward pass\n",
    "\n",
    "# # Convert predictions and tensors back to NumPy\n",
    "# predictions = predictions.cpu().numpy()\n",
    "# X_test_np = X_test.cpu().numpy()\n",
    "# y_test_np = y_test.cpu().numpy()\n",
    "\n",
    "# # Number of test samples\n",
    "# num_tests = X_test.shape[0]\n",
    "\n",
    "# # print(X.shape, y.shape, X_test.shape, y_test.shape, X_train.shape, y_train.shape)\n",
    "\n",
    "# # Loop over each test sample to plot\n",
    "# for i in range(num_tests):\n",
    "#     # Extract strain_11 (component 0 of strain tensor)\n",
    "#     strain_11 = X_test_np[i, :, 1]  # Strain in the first direction (epsilon_11)\n",
    "#     # print(strain_11)\n",
    "    \n",
    "#     # Extract true stress_11 (component 0 of stress tensor)\n",
    "#     true_stress_11 = y_test_np[i, :, 1]  # True stress in the first direction (sigma_11)\n",
    "#     # Extract predicted stress_11 (component 0 of predicted stress tensor)\n",
    "#     predicted_stress_11 = predictions[i, :, 1]  # Predicted stress in the first direction (sigma_11)\n",
    "\n",
    "#     # Compute R² score\n",
    "#     r2 = r2_score(true_stress_11, predicted_stress_11)\n",
    "\n",
    "#     # Plot true stress_11 and predicted stress_11 against strain_11\n",
    "#     plt.figure(figsize=(8, 6))\n",
    "#     plt.plot(strain_11, true_stress_11, label='True Stress_11', color='blue', marker='o')\n",
    "#     plt.plot(strain_11, predicted_stress_11, label='Predicted Stress_11', color='red', linestyle='--')\n",
    "\n",
    "#     # Labeling the plot\n",
    "#     plt.title(f'Test Sample {i+1}: Stress_11 vs Strain_11 (R2 = {r2:.4f})')\n",
    "#     plt.xlabel('Strain_11 (epsilon_11)')\n",
    "#     plt.ylabel('Stress_11 (sigma_11)')\n",
    "#     plt.legend()\n",
    "\n",
    "#     # Show plot\n",
    "#     plt.show()\n",
    "\n",
    "#     # Save plot as an image file\n",
    "#     plt.savefig(f'{save_dir}/plot_example_{i}.png')\n",
    "\n",
    "#     # Close the figure to free memory\n",
    "#     plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e27c93f-3d61-44d0-a0ae-c8aee89d47cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import os\n",
    "# from sklearn.metrics import r2_score\n",
    "\n",
    "# # Ensure model is in evaluation mode\n",
    "# model.eval()\n",
    "\n",
    "# # Create a directory to save plots\n",
    "# save_dir = 'test_set_plots_3'\n",
    "# os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# # Ensure X_test is a PyTorch tensor and move to the correct device\n",
    "# X_train = X_train.to(next(model.parameters()).device)\n",
    "# print(X_train.shape)\n",
    "# # Make predictions for the test set (disable gradients)\n",
    "# with torch.no_grad():\n",
    "#     predictions = model(X_train)  # Forward pass\n",
    "\n",
    "# # Convert predictions and tensors back to NumPy\n",
    "# predictions = predictions.cpu().numpy()\n",
    "# X_train_np = X_train.cpu().numpy()\n",
    "# y_train_np = y_train.cpu().numpy()\n",
    "\n",
    "# # Number of test samples\n",
    "# num_train = X_train.shape[0]\n",
    "# # print(num_tests, X_train)\n",
    "# # Loop over each test sample to plot  \n",
    "# for i in range(num_train):\n",
    "#     # Extract strain_11 (component 0 of strain tensor)\n",
    "#     strain_11 = X_train_np[i, :, 1]  # Strain in the first direction (epsilon_11)\n",
    "#     # print(len( strain_11))\n",
    "#     # Extract true stress_11 (component 0 of stress tensor)\n",
    "#     true_stress_11 = y_train_np[i, :, 1]  # True stress in the first direction (sigma_11)\n",
    "\n",
    "#     # Extract predicted stress_11 (component 0 of predicted stress tensor)\n",
    "#     predicted_stress_11 = predictions[i, :, 1]  # Predicted stress in the first direction (sigma_11)\n",
    "\n",
    "#     # Compute R² score\n",
    "#     r2 = r2_score(true_stress_11, predicted_stress_11)\n",
    "\n",
    "#     # Plot true stress_11 and predicted stress_11 against strain_11\n",
    "#     plt.figure(figsize=(8, 6))\n",
    "#     plt.plot(strain_11, true_stress_11, label='True Stress_11', color='blue', marker='o')\n",
    "#     plt.plot(strain_11, predicted_stress_11, label='Predicted Stress_11', color='red', linestyle='--')\n",
    "\n",
    "#     # Labeling the plot\n",
    "#     plt.title(f'Train Sample {i+1}: Stress_11 vs Strain_11 (R2 = {r2:.4f})')\n",
    "#     plt.xlabel('Strain_11 (epsilon_11)')\n",
    "#     plt.ylabel('Stress_11 (sigma_11)')\n",
    "#     plt.legend()\n",
    "\n",
    "#     # Show plot\n",
    "#     plt.show()\n",
    "\n",
    "#     # Save plot as an image file\n",
    "#     plt.savefig(f'{save_dir}/plot_example_{i}.png')\n",
    "\n",
    "#     # Close the figure to free memory\n",
    "#     plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c69bdf-b3e3-4287-b107-c28ed6c1b150",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6c3170-6016-4879-a4ce-ceca84e99483",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5009e0-04f8-4e6d-8c4f-1c2cc93524a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "# import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a443c29-eda4-4630-9a3b-b74d8e908157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available GPUs\n",
    "physical_devices = torch.cuda.device_count()\n",
    "print(f\"Available GPUs: {physical_devices}\")\n",
    "\n",
    "# Set the GPU to use\n",
    "gpu_id = 4  # Choose the desired GPU index\n",
    "\n",
    "# Ensure the selected GPU exists\n",
    "if gpu_id < physical_devices:\n",
    "    device = torch.device(f\"cuda:{gpu_id}\")\n",
    "    torch.cuda.set_device(device)  # Set the current device\n",
    "    print(f\"Binding to GPU {gpu_id}: {torch.cuda.get_device_name(gpu_id)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"GPU {gpu_id} not available, using CPU instead.\")\n",
    "\n",
    "# Example: Move a tensor to the selected GPU\n",
    "tensor_example = torch.tensor([1.0, 2.0, 3.0]).to(device)\n",
    "print(tensor_example.device)  # Should print \"cuda:4\" if GPU 4 is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9814049b-2c9f-4aeb-aa5f-9c853ab3f730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 文件路径\n",
    "file_paths = [\n",
    "\n",
    "    \"averaged_size_30_strain22.csv\",\n",
    "\n",
    "]\n",
    "\n",
    "# 读取数据\n",
    "df_list = [pd.read_csv(file) for file in file_paths]\n",
    "\n",
    "# 逐个增加索引\n",
    "index_offset = 0\n",
    "for df in df_list:\n",
    "    df['index'] = df['index'] + index_offset\n",
    "    index_offset += len(df['index'].unique())\n",
    "\n",
    "# 合并数据\n",
    "df_combined = pd.concat(df_list, ignore_index=False)\n",
    "\n",
    "# 删除缺失值\n",
    "df_combined.dropna(inplace=True)\n",
    "\n",
    "# 转换数据类型为 float64\n",
    "df_combined = df_combined.astype(np.float64)\n",
    "\n",
    "# 显示前 1001 行\n",
    "print(df_combined.head(1001))\n",
    "df_combined.to_csv(\"combined_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a707d48-d0df-47a1-a43a-d9d133a60971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateRmatrix(angle1, angle2, angle3):\n",
    "    R1 = np.array([[np.cos(angle1), -np.sin(angle1), 0],[np.sin(angle1), np.cos(angle1), 0],[0, 0, 1]])\n",
    "    print(R1.shape)\n",
    "    R2 = np.array([[np.cos(angle2), 0, np.sin(angle2)], [0,1,0], [-np.sin(angle2), 0, np.cos(angle2)]])\n",
    "    R3 = np.array([[1, 0, 0], [0, np.cos(angle3), -np.sin(angle3)], [0, np.sin(angle3), np.cos(angle3)]])\n",
    "    R = np.matmul(np.matmul(R1, R2), R3)\n",
    "    return R\n",
    "\n",
    "R=generateRmatrix(angle1[0], angle2[0], angle3[0])\n",
    "print(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38336031-49e2-4b24-87ce-e14963539589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "sequence_length = 1000  # 每个序列的时间步\n",
    "input_n_features = 6\n",
    "output_n_features = 6\n",
    "\n",
    "input_columns = [\"strain11\", \"strain22\", \"strain33\", \"strain12\", \"strain13\", \"strain23\"]\n",
    "output_columns = [\"stress11\", \"stress22\", \"stress33\", \"stress12\", \"stress13\", \"stress23\"]\n",
    "\n",
    "# 重新计算符合条件的样本数\n",
    "valid_indices = df_combined['index'].unique()\n",
    "count = len(valid_indices)   # 每个 index 有 3 组数据\n",
    "\n",
    "# 初始化 X 和 y 数组\n",
    "X = np.zeros((count, sequence_length, input_n_features))\n",
    "y = np.zeros((count, sequence_length, output_n_features))\n",
    "\n",
    "# 填充 X 和 y\n",
    "count = 0\n",
    "for i in valid_indices:\n",
    "    df1 = df_combined[df_combined['index'] == i]\n",
    "    df1 = df1.sort_values(by=\"step\")  # 确保 step 顺序正确\n",
    "    \n",
    "    # 按 step 递增分成三组，每组取 1000 行\n",
    "    for j in range(3):\n",
    "        subset = df1.iloc[j * sequence_length : (j + 1) * sequence_length]\n",
    "        if len(subset) == sequence_length:\n",
    "            X[count] = subset[input_columns].to_numpy()\n",
    "            y[count] = subset[output_columns].to_numpy()\n",
    "            count += 1\n",
    "\n",
    "# 输出最终 X, y 的形状\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95afe35-68f3-4a47-8ac6-4dd9e591b60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新计算符合条件的样本数\n",
    "count = len(valid_indices)   # 每个 index 有 3 组数据\n",
    "\n",
    "# 初始化 X 和 y 数组\n",
    "X = np.zeros((count, sequence_length, input_n_features))\n",
    "y = np.zeros((count, sequence_length, output_n_features))\n",
    "\n",
    "# 填充 X 和 y\n",
    "count = 0\n",
    "for i in valid_indices:\n",
    "    df1 = df_combined[df_combined['index'] == i]\n",
    "    df1 = df1.sort_values(by=\"step\")  # 确保 step 顺序正确\n",
    "\n",
    "    # 按 step 递增分成三组，每组取 1000 行\n",
    "    for j in range(3):\n",
    "        subset = df1.iloc[j * sequence_length : (j + 1) * sequence_length]\n",
    "        if len(subset) == sequence_length:\n",
    "            # 计算标准差\n",
    "            std_devs_in = subset[input_columns].std().to_numpy()\n",
    "            std_devs_out = subset[output_columns].std().to_numpy()\n",
    "\n",
    "            # 避免除零错误\n",
    "            std_devs_in[std_devs_in == 0] = 1e-6\n",
    "            std_devs_out[std_devs_out == 0] = 1e-6\n",
    "\n",
    "            #归一化数据\n",
    "            X[count] = subset[input_columns].to_numpy() / std_devs_in\n",
    "            y[count] = subset[output_columns].to_numpy() / std_devs_out\n",
    "           #  # Compute normalization parameters\n",
    "           #  X_min = subset[input_columns].min().to_numpy()  # Minimum values for each feature\n",
    "           #  X_max = subset[input_columns].max().to_numpy()  # Maximum values for each feature\n",
    "\n",
    "           #  X_m = (X_min + X_max) / 2  # Mean of min and max\n",
    "           #  X_s = (X_max - X_min) / 2  # Scaling factor\n",
    "\n",
    "           # # Normalize X using the given formula\n",
    "           #  X[count] = (subset[input_columns].to_numpy() - X_m) / X_s\n",
    "\n",
    "           # # Compute normalization parameters for y\n",
    "           #  y_min = subset[output_columns].min().to_numpy()\n",
    "           #  y_max = subset[output_columns].max().to_numpy()\n",
    "\n",
    "           #  y_m = (y_min + y_max) / 2\n",
    "           #  y_s = (y_max - y_min) / 2\n",
    "\n",
    "           #  # Normalize y using the given formula\n",
    "           #  y[count] = (subset[output_columns].to_numpy() - y_m) / y_s\n",
    "           #  # X[count] = subset[input_columns].to_numpy() \n",
    "           #  # y[count] = subset[output_columns].to_numpy() \n",
    "            # Compute normalization parameters\n",
    "            X_min = subset[input_columns].min().to_numpy()  # Minimum values for each feature\n",
    "            X_max = subset[input_columns].max().to_numpy()  # Maximum values for each feature\n",
    "\n",
    "          # Normalize X using min-max normalization\n",
    "            X[count] = (subset[input_columns].to_numpy() - X_min) / (X_max - X_min)\n",
    "\n",
    "         # Compute normalization parameters for y\n",
    "            y_min = subset[output_columns].min().to_numpy()\n",
    "            y_max = subset[output_columns].max().to_numpy()\n",
    "\n",
    "          # Normalize y using min-max normalization\n",
    "            y[count] = (subset[output_columns].to_numpy() - y_min) / (y_max - y_min)\n",
    "\n",
    "            count += 1\n",
    "\n",
    "# 输出最终 X, y 的形状\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f08b6d-8603-4d1b-8ad1-fd57c46dbfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define custom loss function in PyTorch\n",
    "def make_custom_loss_batch(model, X_batch):\n",
    "    def custom_loss(y_pred, y_true):\n",
    "        # Convert R to a PyTorch tensor\n",
    "        R_tensor = torch.tensor(R, dtype=torch.float32, device=y_pred.device)\n",
    "        strain = X_batch.to(dtype=torch.float32)\n",
    "\n",
    "        # Extract the diagonal strain components (first three features)\n",
    "        diagonal_strain = strain[:, :, :3]  # Shape: (batch_size, 1000, 3)\n",
    "\n",
    "        # Compute Term 1: MSE loss between predictions and ground truth\n",
    "        term1 = torch.mean(torch.sum((y_pred - y_true) ** 2, dim=[1, 2]))\n",
    "\n",
    "        # Compute Term 2: Rotation-based transformation\n",
    "        rotated_strain = torch.matmul(diagonal_strain.view(-1, 3), R_tensor)\n",
    "        rotated_strain = rotated_strain.view(diagonal_strain.shape)  # Reshape to original shape\n",
    "\n",
    "        # Compute R^{-1} (inverse of R)\n",
    "        R_tensor_inv = torch.linalg.inv(R_tensor)  \n",
    "        transformed_strain = torch.matmul(rotated_strain.view(-1, 3), R_tensor_inv)\n",
    "        transformed_strain = transformed_strain.view(rotated_strain.shape)\n",
    "\n",
    "        # Pad transformed strain to match the input shape\n",
    "        transformed_strain_padded = torch.cat(\n",
    "            [transformed_strain, torch.zeros_like(strain[:, :, 3:])], dim=-1\n",
    "        )\n",
    "\n",
    "        # Predict stress using the model\n",
    "        predicted_transformed_stress = model(transformed_strain_padded)  \n",
    "\n",
    "        # Apply rotation matrix to stress\n",
    "        rotated_stress = torch.matmul(y_pred[:, :, :3].reshape(-1, 3), R_tensor)\n",
    "        rotated_stress = rotated_stress.view(y_pred[:, :, :3].shape)\n",
    "\n",
    "        # Compute Term 2 difference\n",
    "        difference = predicted_transformed_stress[:, :, :3] - rotated_stress\n",
    "        term2 = torch.mean(torch.sum(difference ** 2, dim=[1, 2]))\n",
    "\n",
    "        # Compute Term 3: Delta stress change\n",
    "        strain_current = strain[:, 1:, :6]  \n",
    "        strain_previous = strain[:, :-1, :6]  \n",
    "        delta_sigma = torch.cat(\n",
    "            [strain_previous[:, :1, :], strain_current - strain_previous], dim=1\n",
    "        )\n",
    "\n",
    "        # Compute stress dot product change\n",
    "        stress_dot_change = torch.sum(y_pred * delta_sigma, dim=[1, 2])\n",
    "        t = 1.0\n",
    "        relu_term = F.relu(-t * stress_dot_change)\n",
    "        term3 = torch.mean(relu_term)\n",
    "\n",
    "        # Print debug information\n",
    "        print(\"term1:\", term1.item(), \"term2:\", term2.item(), \"term3:\", term3.item(), \"sum:\", (term1 + term2 + term3).item())\n",
    "\n",
    "        return term1 + term3  # Return the final loss (excluding term2)\n",
    "\n",
    "    return custom_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3c7fdf-5f9e-4ba8-af3d-66a6cb1e7849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import optuna\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Simulate dataset (Replace with real data)\n",
    "num_samples, sequence_length, num_features = 100, 1000, 6\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "# Define GRU Model\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, dropout1, dropout2):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru1 = nn.GRU(input_dim, hidden_dim1, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(dropout1)\n",
    "        self.gru2 = nn.GRU(hidden_dim1, hidden_dim2, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(dropout2)\n",
    "        self.fc = nn.Linear(hidden_dim2, 6)  # Output layer for regression\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"Input shape to GRU: {x.shape}\")  # Debugging\n",
    "        x, _ = self.gru1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.gru2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc(x) \n",
    "        print(f\"Output shape from GRU: {x.shape}\")  # Debugging\n",
    "        return x\n",
    "\n",
    "# Define Optuna Objective Function\n",
    "def objective(trial):\n",
    "    # Sample hyperparameters\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 16, 64, step=8)\n",
    "    gru_units_1 = trial.suggest_int(\"gru_units_1\", 32, 128, step=16)\n",
    "    gru_units_2 = trial.suggest_int(\"gru_units_2\", 16, 64, step=16)\n",
    "    dropout_1 = trial.suggest_float(\"dropout_1\", 0.1, 0.5, step=0.1)\n",
    "    dropout_2 = trial.suggest_float(\"dropout_2\", 0.1, 0.5, step=0.1)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    # Initialize GRU model\n",
    "    model = GRUModel(input_dim=6, hidden_dim1=gru_units_1, hidden_dim2=gru_units_2,\n",
    "                     dropout1=dropout_1, dropout2=dropout_2).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    epochs = 300\n",
    "    for epoch in range(epochs):\n",
    "        total_loss_epoch = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            # Get the loss function dynamically for this batch\n",
    "            loss_fn = make_custom_loss_batch(model, X_batch)  \n",
    "            loss = loss_fn(y_pred, y_batch)  \n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss_epoch += loss.item()\n",
    "        print(f'Epoch {epoch + 1}, Total Loss: {total_loss_epoch}')\n",
    "    \n",
    "    return total_loss_epoch  # Return final loss for Optuna to minimize\n",
    "\n",
    "# Run Optuna Optimization\n",
    "study = optuna.create_study(direction=\"minimize\")  # Minimize the loss\n",
    "study.optimize(objective, n_trials=10)  # Reduce trials for debugging\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(\"Best hyperparameters:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b8dd06-348c-4c13-9de3-754db0c34ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Simulate dataset (Replace with real data)\n",
    "num_samples, sequence_length, num_features = 100, 1000, 6\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=40)\n",
    "\n",
    "\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "# Define LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, dropout1, dropout2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_dim, hidden_dim1, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(dropout1)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim1, hidden_dim2, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(dropout2)\n",
    "        self.fc = nn.Linear(hidden_dim2, 6)  # Output layer for regression\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc(x)  # Select last timestep for prediction\n",
    "        return x\n",
    "\n",
    "\n",
    "# def train_model(model, train_loader, optimizer, epochs):\n",
    "#     model.train()\n",
    "#     for epoch in range(epochs):\n",
    "#         total_loss_epoch = 0.0\n",
    "#         for X_batch, y_batch in train_loader:\n",
    "#             optimizer.zero_grad()\n",
    "#             y_pred = model(X_batch)\n",
    "#             loss = make_custom_loss_batch(y_pred, y_batch)  # Use custom loss\n",
    "#             # loss.backward()\n",
    "#             optimizer.step()\n",
    "#             total_loss_epoch += loss.item()\n",
    "#         print(f'Epoch {epoch + 1}/{epochs} - Total Loss: {total_loss_epoch}')\n",
    "        \n",
    "def train_model(model, train_loader, optimizer, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss_epoch = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "\n",
    "            # Get the loss function dynamically for this batch\n",
    "            loss_fn = make_custom_loss_batch(model, X_batch)  \n",
    "            loss = loss_fn(y_pred, y_batch)  \n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss_epoch += loss.item()\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}/{epochs} - Total Loss: {total_loss_epoch}')\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 56\n",
    "lstm_units_1 = 112\n",
    "lstm_units_2 = 16\n",
    "dropout_1 = 0.1\n",
    "dropout_2 = 0.3\n",
    "learning_rate = 0.0085\n",
    "epochs = 1000  # Adjust for testing\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "# Initialize Model, Optimizer, Loss Function\n",
    "model = LSTMModel(input_dim=6, hidden_dim1=lstm_units_1, hidden_dim2=lstm_units_2,\n",
    "                  dropout1=dropout_1, dropout2=dropout_2).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "# Train Model\n",
    "train_model(model, train_loader, optimizer, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac6eb1b-b32e-466b-ad97-34dc70fa25ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import os\n",
    "# from sklearn.metrics import r2_score\n",
    "\n",
    "# # Ensure model is in evaluation mode\n",
    "# model.eval()\n",
    "\n",
    "# # Create a directory to save plots\n",
    "# save_dir = 'test_set_plots_3'\n",
    "# os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# # print(X.shape, y.shape, X_test.shape, y_test.shape, X_train.shape, y_train.shape)\n",
    "\n",
    "\n",
    "# # Ensure X_test is a PyTorch tensor and move to the correct device\n",
    "# X_test = X_test.to(next(model.parameters()).device)\n",
    "\n",
    "# # print(X.shape, y.shape, X_test.shape, y_test.shape, X_train.shape, y_train.shape)\n",
    "\n",
    "\n",
    "# # Make predictions for the test set (disable gradients)\n",
    "# with torch.no_grad():\n",
    "#     predictions = model(X_test)  # Forward pass\n",
    "\n",
    "# # Convert predictions and tensors back to NumPy\n",
    "# predictions = predictions.cpu().numpy()\n",
    "# X_test_np = X_test.cpu().numpy()\n",
    "# y_test_np = y_test.cpu().numpy()\n",
    "\n",
    "# # Number of test samples\n",
    "# num_tests = X_test.shape[0]\n",
    "\n",
    "# # print(X.shape, y.shape, X_test.shape, y_test.shape, X_train.shape, y_train.shape)\n",
    "\n",
    "# # Loop over each test sample to plot\n",
    "# for i in range(num_tests):\n",
    "#     # Extract strain_11 (component 0 of strain tensor)\n",
    "#     strain_11 = X_test_np[i, :, 1]  # Strain in the first direction (epsilon_11)\n",
    "#     # print(strain_11)\n",
    "    \n",
    "#     # Extract true stress_11 (component 0 of stress tensor)\n",
    "#     true_stress_11 = y_test_np[i, :, 1]  # True stress in the first direction (sigma_11)\n",
    "#     # Extract predicted stress_11 (component 0 of predicted stress tensor)\n",
    "#     predicted_stress_11 = predictions[i, :, 1]  # Predicted stress in the first direction (sigma_11)\n",
    "\n",
    "#     # Compute R² score\n",
    "#     r2 = r2_score(true_stress_11, predicted_stress_11)\n",
    "\n",
    "#     # Plot true stress_11 and predicted stress_11 against strain_11\n",
    "#     plt.figure(figsize=(8, 6))\n",
    "#     plt.plot(strain_11, true_stress_11, label='True Stress_11', color='blue', marker='o')\n",
    "#     plt.plot(strain_11, predicted_stress_11, label='Predicted Stress_11', color='red', linestyle='--')\n",
    "\n",
    "#     # Labeling the plot\n",
    "#     plt.title(f'Test Sample {i+1}: Stress_11 vs Strain_11 (R2 = {r2:.4f})')\n",
    "#     plt.xlabel('Strain_11 (epsilon_11)')\n",
    "#     plt.ylabel('Stress_11 (sigma_11)')\n",
    "#     plt.legend()\n",
    "\n",
    "#     # Show plot\n",
    "#     plt.show()\n",
    "\n",
    "#     # Save plot as an image file\n",
    "#     plt.savefig(f'{save_dir}/plot_example_{i}.png')\n",
    "\n",
    "#     # Close the figure to free memory\n",
    "#     plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55446282-791f-4786-afa7-81abca5c1092",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4549c1de-9117-457c-a340-6bee78ac09b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb36ad67-be8d-476a-b13f-1b882d02dbae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58c9a6e-0012-4336-a518-8c1368b43be5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6851282-b0fa-4049-85b7-972b1ed2ec55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0204120e-613a-428b-9d46-3f7e38e70d2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575f3f0e-a73e-44d5-b9b8-54bdf0f0cacd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yuhui",
   "language": "python",
   "name": "yuhui"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
